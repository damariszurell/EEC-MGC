<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>SDM algorithms</title>

<script src="site_libs/header-attrs-2.26/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>
<link href="site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Macroecology and global change</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Pracs: macroecological analyses
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="a0_setup.html">0. Getting started</a>
    </li>
    <li>
      <a href="a1_SpatialData.html">1. Spatial data in R</a>
    </li>
    <li>
      <a href="a2_RichnessGradients.html">2. Species richness gradients</a>
    </li>
    <li>
      <a href="a3_RichnessRegression.html">3. Species richness regression</a>
    </li>
    <li>
      <a href="a4_RangeMaps.html">4. Species range maps</a>
    </li>
    <li>
      <a href="a5_SpeciesThreats.html">5. Species threats</a>
    </li>
    <li>
      <a href="a6_BiodivChanges.html">6. Analysing biodiversity changes</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Pracs: species distribution modelling
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="b1_SpeciesData.html">1. Species data</a>
    </li>
    <li>
      <a href="b2_EnvData.html">2. Environmental data</a>
    </li>
    <li>
      <a href="b3_SDM_intro.html">3. SDMs: simple model fitting</a>
    </li>
    <li>
      <a href="b4_SDM_eval.html">4. SDMs: assessment and prediction</a>
    </li>
    <li>
      <a href="b5_pseudoabsence.html">5. Pseudo-absence and background data</a>
    </li>
    <li>
      <a href="b6_SDM_algorithms.html">6. SDMs: algorithms</a>
    </li>
    <li>
      <a href="b7_SDM_ensembles.html">7. SDMs: ensembles</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://www.uni-potsdam.de/en/ibb-macroecology/index">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li>
  <a href="https://twitter.com/ZurellLab">
    <span class="fa fa-twitter"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">SDM algorithms</h1>

</div>


<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('right', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<div class="alert alert-info">
<p><strong>RStudio project</strong></p>
<p>Open the RStudio project that we created in the first session. I
recommend to use this RStudio project for the entire course and within
the RStudio project create separate R scripts for each session.</p>
<ul>
<li>Create a new empty R script by going to the tab “File”, select “New
File” and then “R script”</li>
<li>In the new R script, type <code># Session b6: SDM algorithms</code>
and save the file in your folder “scripts” within your project folder,
e.g. as “b6_SDM_algorithms.R”</li>
</ul>
</div>
<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>So far, we have learnt to fit GLMs to species presence-absence data.
GLMs are only one, very simple parametric method for fitting SDMs. There
are many more algorithms out there <span class="citation">(Elith et al.
2006; Thuiller et al. 2009; Guisan, Thuiller, and Zimmermann
2017)</span>. Here, we will get to know a few of them. Remember the five
general model building steps: (i) conceptualisation, (ii) data
preparation, (iii) model fitting, (iv) assessment and (v) predictions
<span class="citation">(Zurell et al. 2020)</span>. These are the same
for all SDMs independent of the particular algorithm used. In this
tutorial, we will concentrate on model fitting again but will also run
model assessments and make predictions in order to compare the different
algorithms.</p>
<div id="recap-of-last-session-data-and-model-building-steps"
class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Recap of last
session: data and model building steps</h2>
<p>I will illustrate the different algorithms using the Yellowhammer
example that you worked on in practical b3, based on data from the
British breeding and wintering birds citizen science atlas <span
class="citation">(Gillings et al. 2019)</span>. The species
presense/absence data and the bioclimatic variables at these locations
are available from file. We first check for multicollinearity and select
weakly correlated variables based on their univariate variable
importance.</p>
<pre class="r"><code>library(terra)
library(mecofun)

bg &lt;- terra::rast(&#39;data/Prac3_UK_mask.grd&#39;)
sp_dat &lt;- read.table(&#39;data/Prac3_YellowHammer.txt&#39;,header=T)

# Check for multicollinearity
cor_mat &lt;- cor(sp_dat[,-c(1:3)], method=&#39;spearman&#39;)
var_sel &lt;- select07(X=sp_dat[,-c(1:3)], 
                    y=sp_dat$Emberiza_citrinella, 
                    threshold=0.7)

# Inspect weakly correlated variables
var_sel$pred_sel</code></pre>
<pre><code>## [1] &quot;bio12&quot; &quot;bio9&quot;  &quot;bio1&quot;  &quot;bio8&quot;  &quot;bio3&quot;</code></pre>
<pre class="r"><code># Let&#39;s only use the two most important predictors for now
my_preds &lt;- c(&#39;bio12&#39;,&#39;bio9&#39;)</code></pre>
<p>In session 4 on model assessment, we learned that the models should
be validated on indepedent validation data and we have learned how to
run a 5-fold cross-validation. The <code>crossvalSDM()</code> function
could also be used with the algorithms introduced in this practical.
However, to simplify matters let’s rather split the data into training
data and testing data once. For a proper validation this split-sample
should be repeated many times (e.g. n=100). Nevertheless, this unique
split-sample will still give us an idea of model performance and will
allow us to compare the different algorithms.</p>
<pre class="r"><code># First, we randomly select 70% of the rows that will be used as training data
train_i &lt;- sample(seq_len(nrow(sp_dat)), size=round(0.7*nrow(sp_dat)))

# Then, we can subset the training and testing data
sp_train &lt;- sp_dat[train_i,]
sp_test &lt;- sp_dat[-train_i,]

# We store the split information for later:
write(train_i, file=&#39;data/indices_traindata.txt&#39;)</code></pre>
<p>For making predictions in space, we also load the current climate
layers that we downloaded and processed previously. Please remember that
the worldclim layers come in geographic projection in lon/lat format
while the bird data are sampled on the British National Grid. To
harmonise these data, we reprojected the climate data onto the National
Grid.</p>
<pre class="r"><code>bio_curr &lt;- terra::rast(&#39;data/Prac4_UK_bio_curr.tif&#39;)</code></pre>
</div>
</div>
<div id="model-algorithms" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Model algorithms</h1>
<p>Typically, you should decide on appropriate modelling algorithms
during the conceptualisation phase. Let’s assume our study objectives
were to compare species-environment relationships and predicted species
distributions across several SDM algorithms, for example to quantify the
uncertainty due to the model class <span class="citation">(Araujo and
New 2007; Thuiller et al. 2009; Buisson et al. 2010)</span>. We will
test several different SDM algorithms that can be broadly classified
into profile (envelope and distance-based) methods, regression-based
methods and non-parametric machine-learning methods <a
href="http://rspatial.org/sdm/rst/6_sdm_methods.html"><span
class="citation">Guisan, Thuiller, and Zimmermann (2017)</span></a>. The
list of models we treat here is not exhaustive but should give you a
rough overview of what concepts and methods are out there. Most of the
methods used here are available in the package <code>dismo</code>.</p>
<div id="profile-envelope-methods" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Profile (envelope)
methods</h2>
<p>Profile methods constitute the oldest family of SDM algorithms and
are the only “true” presence-only methods that do not need any absence
or background data. We can distinguish the classical envelope approach
and distance-based methods. Here, we only look at the envelope method
BIOCLIM.</p>
<div id="bioclim" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> BIOCLIM</h3>
<p>BIOCLIM is a pioneering envelope approach <span
class="citation">(Booth et al. 2014)</span>. It defines the niche as an
<em>n</em>-dimensional, rectangular bounding box, which is similar to
Hutchinson’s view of the <em>n</em>-dimensional hyperspace <span
class="citation">(Hutchinson 1957)</span>. To reduce sensitivity to
outliers, the bounding box is limited by only considering a certain
percentile range (here, 5-95%) of the species records along each
environmental gradient. In <code>predicts</code>, the BIOCLIM algorithm
is implemented in the function <code>envelope()</code> and will produce
continuous probabilities between 0 and 1, indicating how similar/close
the environmental conditions are to the median conditions.</p>
<p>Previous studies have shown that the BIOCLIM algorithm performs
poorly compared to other methods <span class="citation">(Elith et al.
2006)</span> and should not be used for predicting species distribution
under climate change <span class="citation">(Elith and Graham
2009)</span>. It is still useful in some situations, for example
envelope methods are more prominent in paleoecological studies where
fossil data are sparse.</p>
<pre class="r"><code>library(predicts)

# Fit BIOCLIM model - the algorithm expects a data.frame with environmental variables, but only at the presence locations
m_bc &lt;- predicts::envelope(sp_train[sp_train$Emberiza_citrinella==1,my_preds])
plot(m_bc)</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The resulting plot shows the bounding box (5-95% percentile range).
Green circles outside the bounding box would indicate occurrences that
fall within the 2.5-97.5% percentile range, and the red crosses indicate
occurrences that correspond to extreme environmental values beyond the
2.5-97.5% percentile range. We can also visualise this as response
surface to get a better idea what BIOCLIM is predicting.</p>
<pre class="r"><code># For the response surface, we first prepare the 3D-grid with environmental gradient and predictions
xyz &lt;- expand.grid(
    seq(min(sp_train[,my_preds[1]]),max(sp_train[,my_preds[1]]),length=50),
    seq(min(sp_train[,my_preds[2]]),max(sp_train[,my_preds[2]]),length=50))
names(xyz) &lt;- my_preds
# Make predictions to gradients:
xyz$z &lt;- predict(m_bc, xyz)

# Define colour palette:
library(RColorBrewer)
cls &lt;- colorRampPalette(rev(brewer.pal(11, &#39;RdYlBu&#39;)))(100)

# Plot response surface:
library(lattice)
wireframe(z ~ bio12 + bio9, data = xyz, zlab = list(&quot;Occurrence prob.&quot;, rot=90), 
          drape = TRUE, col.regions = cls, scales = list(arrows = FALSE), 
          zlim = c(0, 1), main=&#39;BIOCLIM&#39;, xlab=&#39;bio12&#39;, ylab=&#39;bio9&#39;, 
          screen=list(z = -120, x = -70, y = 3))</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>We nicely see the median as the peak of the surface, representing the
median environmental conditions in presence locations. Let’s look at the
corresponding partial response plots. The <code>predicts</code> package
offers a function <code>partialResponse()</code> to calculate the
partial response.</p>
<pre class="r"><code># Plot partial response curves:
par(mfrow=c(1,2)) 
plot(partialResponse(m_bc, var=1,nsteps=50),type=&#39;l&#39;, main=&#39;BIOCLIM&#39;, ylab=&#39;Occurrence probability&#39;, xlab=my_preds[1], ylim=c(0,1))
plot(partialResponse(m_bc, var=2,nsteps=50),type=&#39;l&#39;, main=&#39;BIOCLIM&#39;, ylab=&#39;Occurrence probability&#39;, xlab=my_preds[2], ylim=c(0,1))</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Last, we validate model performance on the hold-out test data.</p>
<pre class="r"><code># We use the default MaxSens+Spec threshold:
(perf_bc &lt;- evalSDM(sp_test$Emberiza_citrinella, predict(m_bc, sp_test[,my_preds])))</code></pre>
<pre><code>##         AUC       TSS     Kappa     Sens      Spec       PCC         D2 thresh
## 1 0.7579365 0.4017857 0.4090909 0.547619 0.8541667 0.7111111 -0.3738568  0.175</code></pre>
<p>Finally, let’s map the predicted occurrence probabilities across
Britain and the predicted presence/absence.</p>
<pre class="r"><code># Map predictions
bio_curr_df &lt;- as.data.frame(bio_curr,cell=T,xy=T)
pred_bc_curr &lt;- predict(m_bc, bio_curr_df[,my_preds])
r_bc_pred &lt;- r_bc_bin &lt;- terra::rast(data.frame(bio_curr_df[,c(&#39;x&#39;,&#39;y&#39;)], pred_bc_curr), type=&#39;xyz&#39;)

# Threshold predictions using the maxTSS threshold (max sens+spec)
values(r_bc_bin) &lt;- ifelse(values(r_bc_pred)&gt;=perf_bc$thresh, 1, 0)

# plot the maps
plot(c(r_bc_pred, r_bc_bin),main=c(&#39;BIOCLIM prob.&#39;,&#39;BIOCLIM bin.&#39;), axes=F) </code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
</div>
<div id="regression-based-methods" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Regression-based
methods</h2>
<div id="generalised-linear-models-glms" class="section level3"
number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Generalised linear
models (GLMs)</h3>
<p>We already know GLMs from the previous tutorials. We can fit linear,
quadratic or higher polynomial terms (check <code>poly()</code>) and
interactions between predictors.</p>
<pre class="r"><code># Fit GLM
m_glm &lt;- step(glm( Emberiza_citrinella ~ bio12 + I(bio12^2) + bio9 + I(bio9^2),
    family=&#39;binomial&#39;, data=sp_train))</code></pre>
<pre><code>## Start:  AIC=203.91
## Emberiza_citrinella ~ bio12 + I(bio12^2) + bio9 + I(bio9^2)
## 
##              Df Deviance    AIC
## - I(bio9^2)   1   194.26 202.26
## - bio9        1   194.34 202.34
## - I(bio12^2)  1   194.73 202.73
## &lt;none&gt;            193.91 203.91
## - bio12       1   197.26 205.26
## 
## Step:  AIC=202.26
## Emberiza_citrinella ~ bio12 + I(bio12^2) + bio9
## 
##              Df Deviance    AIC
## - I(bio12^2)  1   194.80 200.80
## - bio9        1   194.81 200.81
## &lt;none&gt;            194.26 202.26
## - bio12       1   197.33 203.33
## 
## Step:  AIC=200.8
## Emberiza_citrinella ~ bio12 + bio9
## 
##         Df Deviance    AIC
## - bio9   1   195.03 199.03
## &lt;none&gt;       194.80 200.80
## - bio12  1   283.89 287.89
## 
## Step:  AIC=199.03
## Emberiza_citrinella ~ bio12
## 
##         Df Deviance    AIC
## &lt;none&gt;       195.03 199.03
## - bio12  1   289.04 291.04</code></pre>
<pre class="r"><code># Now, we plot the response surface:
xyz$z &lt;- predict(m_glm, xyz, type=&#39;response&#39;)

wireframe(z ~ bio12 + bio9, data = xyz, zlab = list(&quot;Occurrence prob.&quot;, rot=90), 
          drape = TRUE, col.regions = cls, scales = list(arrows = FALSE), 
          zlim = c(0, 1), main=&#39;GLM&#39;, xlab=&#39;bio12&#39;, ylab=&#39;bio9&#39;, 
          screen=list(z = -120, x = -70, y = 3))</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code># Plot partial response curves using the mecofun package:
library(mecofun)
par(mfrow=c(1,2)) 
partial_response(m_glm, predictors = sp_train[,my_preds], main=&#39;GLM&#39;, ylab=&#39;Occurrence probability&#39;)</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<pre class="r"><code># Performance measures
(perf_glm &lt;- evalSDM(sp_test$Emberiza_citrinella, predict(m_glm, sp_test[,my_preds], type=&#39;response&#39;) ))</code></pre>
<pre><code>##        AUC       TSS     Kappa      Sens   Spec       PCC        D2 thresh
## 1 0.890377 0.7172619 0.7119645 0.9047619 0.8125 0.8555556 0.3702978  0.525</code></pre>
<pre class="r"><code># Map predictions:
bio_curr_df &lt;- data.frame(crds(bio_curr[[my_preds]]), as.points(bio_curr[[my_preds]]))
r_glm_bin &lt;- r_glm_pred &lt;- terra::rast(cbind(bio_curr_df[,1:2],
                                  predict(m_glm, bio_curr_df, type=&#39;response&#39;)), 
                                  type=&#39;xyz&#39;, crs=crs(bio_curr))
values(r_glm_bin) &lt;- ifelse(values(r_glm_pred)&gt;=perf_glm$thresh, 1, 0)
plot(c(r_glm_pred, r_glm_bin),main=c(&#39;GLM prob.&#39;,&#39;GLM bin.&#39;), axes=F)   </code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-11-3.png" width="672" /></p>
</div>
<div id="generalised-additive-models-gams" class="section level3"
number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Generalised
additive models (GAMs)</h3>
<p>GAMs are a semi-parametric regression method that use data-defined,
non-parametric smoothing functions to fit non-linear species-environment
relationships. GAMs do not fit the response function to all data points
at once, but use a moving-window approach to fit a local smoother to a
proportion of the data. Small window sizes will yield highly flexible
response shapes while large window sizes will produce less flexible
response shapes that are closer to a parametric GLM. Different packages
for fitting GAMs are available in R. Here, we will use the <em>mgcv</em>
package.</p>
<pre class="r"><code>library(mgcv)

# Fit GAM with spline smoother
m_gam &lt;- mgcv::gam( Emberiza_citrinella ~ s(bio12,k=4) + s(bio9, k=4),
    family=&#39;binomial&#39;, data=sp_train)

# Now, we plot the response surface:
xyz$z &lt;- predict(m_gam, xyz[,1:2], type=&#39;response&#39;)
wireframe(z ~ bio12 + bio9, data = xyz, zlab = list(&quot;Occurrence prob.&quot;, rot=90), 
          drape = TRUE, col.regions = cls, scales = list(arrows = FALSE), 
          zlim = c(0, 1), main=&#39;GAM&#39;, xlab=&#39;bio12&#39;, ylab=&#39;bio9&#39;, 
          screen=list(z = -120, x = -70, y = 3))</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code># Plot partial response curves:
par(mfrow=c(1,2)) 
partial_response(m_gam, predictors = sp_train[,my_preds], main=&#39;GAM&#39;, ylab=&#39;Occurrence probability&#39;)</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-12-2.png" width="672" /></p>
<pre class="r"><code># Performance measures
(perf_gam &lt;- evalSDM(sp_test$Emberiza_citrinella, predict(m_gam, sp_test[,my_preds], type=&#39;response&#39;) ))</code></pre>
<pre><code>##         AUC       TSS     Kappa      Sens   Spec       PCC        D2 thresh
## 1 0.8859127 0.7172619 0.7119645 0.9047619 0.8125 0.8555556 0.3654815   0.53</code></pre>
<pre class="r"><code># Map predictions (the data frame bio_curr_df was defined previously):
r_gam_bin &lt;- r_gam_pred &lt;- terra::rast(cbind(bio_curr_df[,1:2],
                                   predict(m_gam, bio_curr_df, type=&#39;response&#39;)),
                                   type=&#39;xyz&#39;, crs=crs(bio_curr))
values(r_gam_bin) &lt;- ifelse(values(r_gam_pred)&gt;=perf_gam$thresh, 1, 0)
plot(c(r_gam_pred, r_gam_bin),
     main=c(&#39;GAM prob.&#39;,&#39;GAM bin.&#39;), axes=F)    </code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-12-3.png" width="672" /></p>
</div>
</div>
<div id="machine-learning-methods" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Machine-learning
methods</h2>
<p>There are a number of different non-parametric machine-learning
methods that are commonly used in SDMs, and new methods are constantly
appearing. A few methods like Classification and Regression Trees (CART)
and Artificial Neural Networks (ANN) have been around for some time,
while other methods such as Boosted Regression Trees (BRTs), Random
Forests (RFs) and Maximum Entropy (MaxEnt) have only become popular over
the last decade.</p>
<div id="classification-and-regression-trees-cart"
class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Classification and
regression trees (CART)</h3>
<p>CARTs are a recursive partitioning method that aim to divide the data
into homogeneous subgroups <span class="citation">(Franklin 2010;
Guisan, Thuiller, and Zimmermann 2017)</span>. They grow a decision tree
by repeatedly splitting the data such that the splits help separating
presences and absences. Thus, CARTs search along each environmental
gradient for those splitting rules (nodes) that best separate the
observations. Of course, we could perfectly fit all data by that
procedure, which is rarely desirable as it will decrease the bias for
the training data but increase the variance for a different sample (the
<em>bias-variance tradeoff</em>). Thus, the procedure is basically to
grow the tree, stop the tree and prune the tree to find the optimal tree
size.</p>
<p>Again, different packages are available for fitting CARTs,
e.g. <em>rpart</em> and <em>tree</em>. The package <em>rpart</em> offers
better control of model fitting and tree size optimization. It uses
internal cross-validation (default <code>xval=10</code>) for evaluating
bias-variance tradeoff and optimizing tree size. Another important
control parameter is the minimum number of observations (default
<code>minsplit=20</code>) that must be available to define a split.</p>
<pre class="r"><code>library(rpart)

# Fit CART
m_cart &lt;- rpart(Emberiza_citrinella ~ bio12 + bio9,
    data=sp_train, control=rpart.control(minsplit=20,xval=10))
    
# Have a look at the data splits:
print(m_cart)</code></pre>
<pre><code>## n= 211 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 211 51.8862600 0.56398100  
##    2) bio12&gt;=1233.15 63  5.4285710 0.09523810  
##      4) bio9&lt; 9.851603 44  0.9772727 0.02272727 *
##      5) bio9&gt;=9.851603 19  3.6842110 0.26315790 *
##    3) bio12&lt; 1233.15 148 26.7229700 0.76351350  
##      6) bio12&gt;=875.9535 41 10.1951200 0.53658540  
##       12) bio9&gt;=10.09972 29  7.1724140 0.44827590  
##         24) bio12&lt; 1004.628 10  1.6000000 0.20000000 *
##         25) bio12&gt;=1004.628 19  4.6315790 0.57894740 *
##       13) bio9&lt; 10.09972 12  2.2500000 0.75000000 *
##      7) bio12&lt; 875.9535 107 13.6074800 0.85046730 *</code></pre>
<pre class="r"><code>plot(m_cart, margin=0.1)
text(m_cart, cex = 0.8)</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code># Now, we plot the response surface:
xyz$z &lt;- predict(m_cart, xyz)
wireframe(z ~ bio12 + bio9, data = xyz, zlab = list(&quot;Occurrence prob.&quot;, rot=90), 
          drape = TRUE, col.regions = cls, scales = list(arrows = FALSE), 
          zlim = c(0, 1), main=&#39;CART&#39;, xlab=&#39;bio12&#39;, ylab=&#39;bio9&#39;, 
          screen=list(z = -120, x = -70, y = 3))</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-13-2.png" width="672" /></p>
<pre class="r"><code># Plot partial response curves:
par(mfrow=c(1,2)) 
partial_response(m_cart, predictors = sp_train[,my_preds], main=&#39;CART&#39;, ylab=&#39;Occurrence probability&#39;)</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-13-3.png" width="672" /></p>
<pre class="r"><code># Performance measures of GAM with spline-smoother 
(perf_cart &lt;- evalSDM(sp_test$Emberiza_citrinella, predict(m_cart, sp_test[,my_preds]) ))</code></pre>
<pre><code>##         AUC       TSS     Kappa      Sens      Spec PCC       D2 thresh
## 1 0.8196925 0.5922619 0.5958084 0.7380952 0.8541667 0.8 0.169344    0.8</code></pre>
<pre class="r"><code># Map predictions:
r_cart_bin &lt;- r_cart_pred &lt;- terra::rast(cbind(bio_curr_df[,1:2],
                                   predict(m_cart, bio_curr_df)),
                                   type=&#39;xyz&#39;, crs=crs(bio_curr))
values(r_cart_bin) &lt;- ifelse(values(r_cart_pred)&gt;=perf_cart$thresh, 1, 0)
plot(c(r_cart_pred, r_cart_bin),main=c(&#39;CART prob.&#39;,&#39;CART bin.&#39;), axes=F)   </code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-13-4.png" width="672" /></p>
</div>
<div id="random-forests-rfs" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Random Forests
(RFs)</h3>
<p>Regression models or classification models can be affected by local
optima and noise in the data. They usually have low bias (fit the
training data very well) but high variance (noisy/poorer performance
when predicting to non-training data). Model averaging has been proposed
as possible solution <span class="citation">(Hastie, Tibshirani, and
Friedman 2009)</span>. In recent years, so-called bagging and boosting
methods have been developed for combining or averaging different models.
Random Forests use a bagging procedure for averaging the outputs of a
multitude of different CARTs. Bagging stands for “bootstrap
aggregation”. Basically, we fit many CARTs to bootstrapped samples of
the training data and then either average the results in case of
regression trees or make a simple vote in case of classification trees
(committee averaging)<span class="citation">(Hastie, Tibshirani, and
Friedman 2009; Guisan, Thuiller, and Zimmermann 2017)</span>. An
important feature of Random Forests are the out-of-bag samples, which
means that the predictions/fit for a specific data point is only derived
from averaging trees that did not include this data point during tree
growing. Thus, the output of Random Forests is essentially
cross-validated. Random Forests estimate variable importance by a
permutation procedure, which measures for each variable the drop in mean
accuracy when this variables is permutated.</p>
<pre class="r"><code>library(randomForest)

# Fit RF (a question/warning pops up whether we really want to do regression: YES, we want to. With binary data, you could also use classification by setting the argument y=as.factor(sp_train$Emberiza_citrinella) but from experience regression produces slightly better results)
m_rf &lt;- randomForest( x=sp_train[,my_preds], y=sp_train$Emberiza_citrinella, 
    ntree=1000, importance =T)
    
# Variable importance:
importance(m_rf,type=1)</code></pre>
<pre><code>##        %IncMSE
## bio12 71.11887
## bio9  15.49371</code></pre>
<pre class="r"><code>varImpPlot(m_rf)</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code># Look at single trees:
head(getTree(m_rf,1,T))</code></pre>
<pre><code>##   left daughter right daughter split var split point status prediction
## 1             2              3     bio12 1065.103210     -3 0.56398104
## 2             4              5     bio12  568.039551     -3 0.83458647
## 3             6              7      bio9   12.342194     -3 0.10256410
## 4             8              9      bio9    6.123576     -3 0.42857143
## 5            10             11      bio9    7.524390     -3 0.85714286
## 6            12             13      bio9    8.596580     -3 0.08333333</code></pre>
<pre class="r"><code># Now, we plot the response surface:
xyz$z &lt;- predict(m_rf, xyz, type=&#39;response&#39;)
wireframe(z ~ bio12 + bio9, data = xyz, zlab = list(&quot;Occurrence prob.&quot;, rot=90), 
          drape = TRUE, col.regions = cls, scales = list(arrows = FALSE), 
          zlim = c(0, 1), main=&#39;Random Forest&#39;, xlab=&#39;bio12&#39;, ylab=&#39;bio9&#39;, 
          screen=list(z = -120, x = -70, y = 3))</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-14-2.png" width="672" /></p>
<pre class="r"><code># Plot partial response curves:
par(mfrow=c(1,2)) 
partial_response(m_rf, predictors = sp_train[,my_preds], main=&#39;Random Forest&#39;, ylab=&#39;Occurrence probability&#39;)</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-14-3.png" width="672" /></p>
<pre class="r"><code># Performance measures of RF
(perf_rf &lt;- evalSDM(sp_test$Emberiza_citrinella, predict(m_rf, sp_test[,my_preds],  type=&#39;response&#39;) ))</code></pre>
<pre><code>##         AUC       TSS     Kappa      Sens      Spec       PCC        D2 thresh
## 1 0.8343254 0.6428571 0.6428571 0.8095238 0.8333333 0.8222222 0.1757222   0.62</code></pre>
<pre class="r"><code># Map predictions:
r_rf_bin &lt;- r_rf_pred &lt;- terra::rast(cbind(bio_curr_df[,1:2],
                                 predict(m_rf, bio_curr_df,type=&#39;response&#39;)),
                                 type=&#39;xyz&#39;, crs=crs(bio_curr))
values(r_rf_bin) &lt;- ifelse(values(r_rf_pred)&gt;=perf_rf$thresh, 1, 0)
plot(c(r_rf_pred, r_rf_bin),main=c(&#39;RF prob.&#39;,&#39;RF bin.&#39;), axes=F)   </code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-14-4.png" width="672" /></p>
</div>
<div id="boosted-regression-trees-brts" class="section level3"
number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Boosted regression
trees (BRTs)</h3>
<p>Boosting is another averaging/ensemble approach for improving the
predictive performance of models <span class="citation">(Hastie,
Tibshirani, and Friedman 2009; Guisan, Thuiller, and Zimmermann
2017)</span>. Boosting of CARTS is known under different names including
Gradient Boosting Machine (GBM), Generalised Boosted Regression Model
(GBM) and Boosted Regression Trees (BRTs) among others. In R it is
available in the package <em>gbm</em> with some additional functions
from <span class="citation">Elith, Leathwick, and Hastie (2008)</span>
provided in the <em>dismo</em> package. <span class="citation">Elith,
Leathwick, and Hastie (2008)</span> also provide a working guide for
using BRTs in species distribution modelling. Unlike Random Forests,
BRTs iteratively fit relatively simple trees by putting emphasis on
observations fitted poorly by the previous trees (by fitting the new
tree to the residuals of the previous tree). The final BRT can be
thought of as linear combination of all trees, similar to a regression
model where each term is a single tree <span class="citation">(Elith,
Leathwick, and Hastie 2008)</span>. Thereby each tree is shrunk by the
learning rate (the shrinkage parameter, typically &lt;1), which
determines how much weight is given to single trees. Generally, slower
learning (meaning smaller learning rates) are preferable. Similarly to
Random Forests, only a subset of the data (the <em>bag fraction</em>) is
used for fitting consecutive trees (but in contrast to Random Forests,
the subsets are sampled without replacement and thus constitute real
data splits). This <em>bag fraction</em> should typically range 0.5-0.75
<span class="citation">(Elith, Leathwick, and Hastie 2008)</span>. The
tree complexity controls the interaction depth; <code>1</code> means
only tree stumps (with two terminal nodes) are fitted, <code>2</code>
means a model with up to two-way interactions etc. In the regular
<code>gbm()</code>function, you have to define the maximum number of
trees fitted. <span class="citation">Elith, Leathwick, and Hastie
(2008)</span> recommend fitting at least 1000 trees. However, you want
to be careful not to overfit the model by fitting too many trees. The
<em>dismo</em> package provides the function <code>gbm.step</code> that
selects the optimum number of trees based on the reduction in deviance
achieved by adding a tree while predicting to the hold-out data
(1-<code>bag fraction</code>). If the optimal number of trees estimated
by the model is below 1000, you should decrease your learning rate; if
it is above 10000, you should increase your learning rate. A tutorial on
BRTs is contained in the dismo package: <code>vignette('brt')</code></p>
<pre class="r"><code>library(dismo)
library(gbm)

# Fit BRT
m_brt &lt;- gbm.step(data = sp_train, 
    gbm.x = my_preds,
    gbm.y = &#39;Emberiza_citrinella&#39;, 
    family = &#39;bernoulli&#39;,
    tree.complexity = 2,
    bag.fraction = 0.75,
    learning.rate = 0.001,
    verbose=F)</code></pre>
<pre><code>## 
##  
##  GBM STEP - version 2.9 
##  
## Performing cross-validation optimisation of a boosted regression tree model 
## for NA and using a family of bernoulli 
## Using 211 observations and 2 predictors 
## creating 10 initial models of 50 trees 
## 
##  folds are stratified by prevalence 
## total mean deviance =  1.3699 
## tolerance is fixed at  0.0014 
## now adding trees...</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code># Variable importance:
m_brt$contributions</code></pre>
<pre><code>##         var   rel.inf
## bio12 bio12 93.369436
## bio9   bio9  6.630564</code></pre>
<pre class="r"><code># Interactions (not very meaningful here with only 2 predictors):
gbm.interactions(m_brt)$interactions</code></pre>
<pre><code>##       bio12  bio9
## bio12     0 24.08
## bio9      0  0.00</code></pre>
<pre class="r"><code>gbm.interactions(m_brt)$rank.list</code></pre>
<pre><code>##   var1.index var1.names var2.index var2.names int.size
## 1          2       bio9          1      bio12    24.08
## 2          3       &lt;NA&gt;          0                0.00</code></pre>
<pre class="r"><code># dismo provides some build-in functions for plotting response:
gbm.plot(m_brt, n.plots=2, write.title = FALSE)</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-15-2.png" width="672" /></p>
<pre class="r"><code>gbm.plot.fits(m_brt)</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-15-3.png" width="672" /></p>
<pre class="r"><code># Now, we plot the response surface:
xyz$z &lt;- predict.gbm(m_brt, xyz, n.trees=m_brt$gbm.call$best.trees, type=&quot;response&quot;)
wireframe(z ~ bio12 + bio9, data = xyz, zlab = list(&quot;Occurrence prob.&quot;, rot=90), 
          drape = TRUE, col.regions = cls, scales = list(arrows = FALSE), 
          zlim = c(0, 1), main=&#39;Boosted regression trees&#39;, xlab=&#39;bio12&#39;, 
          ylab=&#39;bio9&#39;, screen=list(z = -120, x = -70, y = 3))</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-15-4.png" width="672" /></p>
<pre class="r"><code># Plot partial response curves:
par(mfrow=c(1,2)) 
partial_response(m_brt, predictors = sp_train[,my_preds], main=&#39;BRT&#39;, ylab=&#39;Occurrence probability&#39;)</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-15-5.png" width="672" /></p>
<pre class="r"><code># Performance measures of BRT
(perf_brt &lt;- evalSDM(sp_test$Emberiza_citrinella, predict.gbm(m_brt, sp_test[,my_preds], n.trees=m_brt$gbm.call$best.trees, type=&#39;response&#39;) ))</code></pre>
<pre><code>##         AUC       TSS     Kappa      Sens      Spec       PCC        D2 thresh
## 1 0.8831845 0.7380952 0.7337278 0.9047619 0.8333333 0.8666667 0.3442508    0.6</code></pre>
<pre class="r"><code># Map predictions:
r_brt_bin &lt;- r_brt_pred &lt;- terra::rast(cbind(bio_curr_df[,1:2],
                                  predict.gbm(m_brt, bio_curr_df,
                                              n.trees=m_brt$gbm.call$best.trees, 
                                              type=&quot;response&quot;)),
                                  type=&#39;xyz&#39;, crs=crs(bio_curr))
values(r_brt_bin) &lt;- ifelse(values(r_brt_pred)&gt;=perf_brt$thresh, 1, 0)
plot(c(r_brt_pred, r_brt_bin),main=c(&#39;BRT prob.&#39;,&#39;BRT bin.&#39;), axes=F)   </code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-15-6.png" width="672" /></p>
</div>
<div id="maxent" class="section level3" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> Maxent</h3>
<p>In recent years, maximum entropy modelling of species distributions
has become very popular and proved as one of the best-performing methods
in model comparisons <span class="citation">Elith et al. (2006)</span>.
The implementation <em>Maxent</em> <span class="citation">(Phillips,
Anderson, and Schapire 2006; Elith et al. 2011; Merow, Smith, and
Silander Jr 2013)</span> now constitutes the most widely used SDM
algorithm. It was originally offered with a stand-alone Java package
with user interface (possibly one reasons for its popularity). Since a
couple of years, some packages like <em>dismo</em> have build-in
functions to communicate with this Maxent programme. Very recently,
Steven Phillips (“Mr. Maxent”) introduced a true R version of Maxent,
the <em>maxnet</em> package.</p>
<p>Maxent is a presence-only method, but unlike profile methods it uses
background data (where presence is unknown) against which it contrasts
the observed presences. <span class="citation">Elith et al.
(2011)</span> provide a simplified explanation of Maxent (while the
original paper by <span class="citation">Phillips, Anderson, and
Schapire (2006)</span> focused on explanations from machine-learning
perspective) stating that Maxent aims to minimise the relative entropy
between the probability density of presences and the probability density
of the environment estimated in environmental (not geographic!) space.
The density of available background data in environmental/covariate
space can be regarded as the null model that assumes that the species
will occupy environmental conditions proportional to their relative
availability in the landscape <span class="citation">(Guisan, Thuiller,
and Zimmermann 2017)</span>. Maxent allows fitting very complex, highly
non-linear response shapes <span class="citation">(Merow, Smith, and
Silander Jr 2013)</span>, defined by so-called feature classes. Maxent
currently recognises six features classes, which are described in more
detail by <span class="citation">Elith et al. (2011)</span> and <span
class="citation">Merow, Smith, and Silander Jr (2013)</span>: linear,
product, quadratic, hinge, threshold and categorical. We already know
linear and quadratic features from GLMs. <em>Products</em> allow simple
interactions between all possible pair-wise combinations of predictor
variables. <em>Thresholds</em> allow a step in the fitted function (as
we have seen in CARTs) and make a continuous predictor binary assigning
0 below the threshold and 1 above the threshold. <em>Hinge</em> features
are similar to <em>thresholds</em> only that they do not fit abrupt
steps but a change in the gradient of the response (a bit like piecewise
linear splines). <em>Categorical</em> features split a predictor with
<span class="math inline">\(n\)</span> categories (such as land cover)
into <span class="math inline">\(n\)</span> binary features assigning 1
when the feature is expressed and 0 otherwise. If the data contain more
than 80 presences, then Maxent will by default use all feature classes
in model fitting (otherwise it will automatically determine the number
of features based on the number of presences). This can easily lead to
more features that are explored in the model than actual presences <span
class="citation">(Merow, Smith, and Silander Jr 2013)</span>. Of course,
users can also specify features themselves. Generally, the selection of
features should be guided by ecological plausibility and be considered
during model conceptualisation. During model fitting, Maxent will select
features based on regularization (trading-off likelihood and model
complexity) to avoid overfitting.</p>
<p>As the density of presence points in environmental space is
contrasted against all available environments, choosing the background
data can be quite crucial in Maxent and should be guided by the spatial
scale of the ecological question <span class="citation">(Merow, Smith,
and Silander Jr 2013)</span>. For example, the geographic extent of
background data should only encompass areas that are accessible by
dispersal and which the species is equally likely to reach. If there is
reason to assume that the presence data are spatially biased, then this
should also be considered when deriving background data, for example by
inducing the same spatial bias in the background <span
class="citation">(Kramer-Schadt et al. 2013)</span>. Here, we avoid the
background data issue as we have true absence data available.</p>
<pre class="r"><code>library(maxnet)

# Fit Maxent
m_maxent &lt;- maxnet(p=sp_train$Emberiza_citrinella, data=sp_train[,my_preds],
    maxnet.formula(p=sp_train$Emberiza_citrinella, data=sp_train[,my_preds], classes=&quot;lh&quot;))

# HINT: try playing around with the classes=&quot;lqpht&quot; argument

# Now, we plot the response surface:
xyz$z &lt;- predict(m_maxent, xyz, type=&quot;logistic&quot;)
wireframe(z ~ bio12 + bio9, data = xyz, zlab = list(&quot;Occurrence prob.&quot;, rot=90), 
          drape = TRUE, col.regions = cls, scales = list(arrows = FALSE), 
          zlim = c(0, 1), main=&#39;Maxent&#39;, xlab=&#39;bio12&#39;, ylab=&#39;bio9&#39;, 
          screen=list(z = -120, x = -70, y = 3))</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre class="r"><code># Plot partial response curves:
par(mfrow=c(1,2)) 
partial_response(m_maxent, predictors = sp_train[,my_preds], main=&#39;Maxent&#39;, ylab=&#39;Occurrence probability&#39;)</code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-16-2.png" width="672" /></p>
<pre class="r"><code># Performance measures of Maxent
(perf_maxent &lt;- evalSDM(sp_test$Emberiza_citrinella, predict(m_maxent, sp_test[,my_preds],  type=&#39;logistic&#39;) ))</code></pre>
<pre><code>##         AUC       TSS     Kappa      Sens   Spec       PCC        D2 thresh
## 1 0.8774802 0.7172619 0.7119645 0.9047619 0.8125 0.8555556 0.2581671   0.41</code></pre>
<pre class="r"><code># Map predictions:
r_maxent_bin &lt;- r_maxent_pred &lt;- terra::rast(cbind(bio_curr_df[,1:2],
                                     predict(m_maxent, bio_curr_df, type=&quot;logistic&quot;)),
                                     type=&#39;xyz&#39;, crs=crs(bio_curr))
values(r_maxent_bin) &lt;- ifelse(values(r_maxent_pred)&gt;=perf_maxent$thresh, 1, 0)
plot(c(r_maxent_pred, r_maxent_bin),main=c(&#39;Maxent prob.&#39;,&#39;Maxent bin.&#39;), axes=F)   </code></pre>
<p><img src="b6_SDM_algorithms_files/figure-html/unnamed-chunk-16-3.png" width="672" /></p>
</div>
</div>
<div id="comparing-all-algorithms" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Comparing all
algorithms</h2>
<p>We can now compare the performance of all algorithms on our
independent data. Also, we store this information for running the
ensembles in the next tutorial.</p>
<pre class="r"><code>(comp_perf &lt;- rbind(bc = perf_bc, glm = perf_glm, gam = perf_gam,
      cart = perf_cart, rf = perf_rf, brt = perf_brt, maxent = perf_maxent))</code></pre>
<pre><code>##              AUC       TSS     Kappa      Sens      Spec       PCC         D2
## bc     0.7579365 0.4017857 0.4090909 0.5476190 0.8541667 0.7111111 -0.3738568
## glm    0.8903770 0.7172619 0.7119645 0.9047619 0.8125000 0.8555556  0.3702978
## gam    0.8859127 0.7172619 0.7119645 0.9047619 0.8125000 0.8555556  0.3654815
## cart   0.8196925 0.5922619 0.5958084 0.7380952 0.8541667 0.8000000  0.1693440
## rf     0.8343254 0.6428571 0.6428571 0.8095238 0.8333333 0.8222222  0.1757222
## brt    0.8831845 0.7380952 0.7337278 0.9047619 0.8333333 0.8666667  0.3442508
## maxent 0.8774802 0.7172619 0.7119645 0.9047619 0.8125000 0.8555556  0.2581671
##        thresh
## bc      0.175
## glm     0.525
## gam     0.530
## cart    0.800
## rf      0.620
## brt     0.600
## maxent  0.410</code></pre>
<pre class="r"><code># We add a column containing the names of the algorithm
comp_perf &lt;- data.frame(alg=row.names(comp_perf),comp_perf)

# Adapt the file path to your folder structure
write.table(comp_perf, file=&#39;data/SDM_alg_performances.txt&#39;, row.names=F)</code></pre>
</div>
</div>
<div id="homework-prep" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Homework prep</h1>
<p>As homework, solve the exercises in the blue box below.</p>
<div class="alert alert-info">
<p><em><strong>Exercise:</strong></em></p>
<p>Use the species-climate data that you prepared for your own species
(from the previous practical b5) and fit SDMs using at least three
different algorithms. Compare the models, which one performs best?</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-araujo2007" class="csl-entry">
Araujo, M. B., and M. New. 2007. <span>“Ensemble Forecasting of Species
Distributions.”</span> <em>Trends in Ecology and Evolution</em> 22:
42–47.
</div>
<div id="ref-Booth2014" class="csl-entry">
Booth, T. H., H. A. Nix, J. R. Busby, and M. F. Hutchinson. 2014.
<span>“Bioclim: The First Species Distribution Modelling Package, Its
Early Applications and Relevance to Most Current MaxEnt Studies.”</span>
<em>Diversity and Distributions</em> 20: 1–9.
</div>
<div id="ref-BUISSON2010" class="csl-entry">
Buisson, L., W. Thuiller, N. Casajus, S. Lek, and G. Grenouillet. 2010.
<span>“Uncertainty in Ensemble Forecasting of Species
Distribution.”</span> <em>Global Change Biology</em> 16: 1145–57.
</div>
<div id="ref-Elith2009" class="csl-entry">
Elith, J., and C. H. Graham. 2009. <span>“Do They? How Do They? WHY Do
They Differ? On Finding Reasons for Differing Performances of Species
Distribution Models.”</span> <em>Ecography</em> 32 (1): 66–77. <a
href="https://doi.org/10.1111/j.1600-0587.2008.05505.x">https://doi.org/10.1111/j.1600-0587.2008.05505.x</a>.
</div>
<div id="ref-elith2006" class="csl-entry">
Elith, J., C. H. Graham, R. P. Anderson, M. Dudik, S. Ferrier, A.
Guisan, R. J. Hijmans, et al. 2006. <span>“Novel Methods Improve
Prediction of Species’ Distribution from Occurence Data.”</span>
<em>Ecography</em> 29: 129–51.
</div>
<div id="ref-elith2008" class="csl-entry">
Elith, J., J. R. Leathwick, and T. Hastie. 2008. <span>“A Working Guide
to Boosted Regression Trees.”</span> <em>Journal of Animal Ecology</em>
77: 802–13.
</div>
<div id="ref-Elith2011" class="csl-entry">
Elith, J., S. J. Phillips, T. Hastie, M. Dudik, Y. E. Chee, and C. J.
Yates. 2011. <span>“A Statistical Explanation of MaxEnt for
Ecologists.”</span> <em>Diversity and Distributions</em> 17: 43–57.
</div>
<div id="ref-Franklin2010" class="csl-entry">
Franklin, J. 2010. <em>Mapping Species Distributions: Spatial Inference
and Prediction</em>. Cambride University Press.
</div>
<div id="ref-Gillings2019" class="csl-entry">
Gillings, S., D. E. Balmer, B. J. Caffrey, I. S. Downie, D. W. Gibbons,
P. C. Lack, J. B. Reid, J. T. R. Sharrock, R. L. Swann, and R. J.
Fuller. 2019. <span>“Breeding and Wintering Bird Distributions in
Britain and Ireland from Citizen Science Bird Atlases.”</span>
<em>Global Ecology and Biogeography</em> 28 (7): 866–74. <a
href="https://doi.org/10.1111/geb.12906">https://doi.org/10.1111/geb.12906</a>.
</div>
<div id="ref-Guisan2017" class="csl-entry">
Guisan, A., W. Thuiller, and N. E. Zimmermann. 2017. <em>Habitat
Suitability and Distribution Models with Applications in r</em>.
Cambride University Press.
</div>
<div id="ref-Hastie2009" class="csl-entry">
Hastie, T., R. Tibshirani, and J. Friedman. 2009. <em>The Elements of
Statistical Learning</em>. Springer.
</div>
<div id="ref-hutchinson1957" class="csl-entry">
Hutchinson, G. E. 1957. <span>“Concluding Remarks, Cold Spring Harbor
Symposium.”</span> <em>Quantitative Biology</em> 22: 415–27.
</div>
<div id="ref-Kramer-Schadt2013" class="csl-entry">
Kramer-Schadt, S., J. Niedballa, J. D. Pilgrim, B. Schroeder, J.
Lindenborn, V. Reinfelder, M. Stillfried, et al. 2013. <span>“The
Importance of Correcting for Sampling Bias in MaxEnt Species
Distribution Models.”</span> <em>Diversity and Distributions</em> 19
(11): 1366–79. <a
href="https://doi.org/10.1111/ddi.12096">https://doi.org/10.1111/ddi.12096</a>.
</div>
<div id="ref-Merow2013" class="csl-entry">
Merow, C., M. J. Smith, and J. A. Silander Jr. 2013. <span>“A Practical
Guide to MaxEnt for Modeling Species’ Distributions: What It Does, and
Why Inputs and Settings Matter.”</span> <em>Ecography</em> 36: 1058–69.
</div>
<div id="ref-phillips2006" class="csl-entry">
Phillips, S. J., R. P. Anderson, and R. E. Schapire. 2006.
<span>“Maximum Entropy Modeling of Species Geographic
Distributions.”</span> <em>Ecological Modelling</em> 190: 231–59.
</div>
<div id="ref-Thuiller2009" class="csl-entry">
Thuiller, W., B. Lafourcade, R. Engler, and M. B. Araujo. 2009.
<span>“BIOMOD - a Platform for Ensemble Forecasting of Species
Distributions.”</span> <em>Ecography</em> 32: 369–73.
</div>
<div id="ref-Zurell2020a" class="csl-entry">
Zurell, D., J. Franklin, C. König, P. J. Bouchet, C. F. Dormann, J.
Elith, G. Fandos, et al. 2020. <span>“A Standard Protocol for Reporting
Species Distribution Models.”</span> <em>Ecography</em> 43 (9): 1261–77.
<a
href="https://doi.org/10.1111/ecog.04960">https://doi.org/10.1111/ecog.04960</a>.
</div>
</div>
</div>

<!DOCTYPE html>
<html>

<br>
<hr />
<div id="footer">
<p>Damaris Zurell 2022 <a href="http://creativecommons.org/licenses/by/4.0/" >(CC BY 4.0)</a>.  </p>
</div>

</html>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
